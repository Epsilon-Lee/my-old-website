<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Zen of Epsilon Lee</title>
    <link rel="stylesheet" href="../stylesheets/styles.css">
    <link rel="stylesheet" href="../stylesheets/github-dark.css">
    <script src="https://code.jquery.com/jquery-3.1.0.min.js"></script>
    <script src="javascripts/main.js"></script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>

    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>

      <header>
        <h1>Thought Zen</h1>
        <p>Sand and Diamond from My Life</p>
      </header>

      <div id="banner">
        <span id="logo"></span>

        <a href="https://github.com/Epsilon-Lee" class="button fork"><strong>View On GitHub</strong></a>
      </div><!-- end banner -->

    <div class="wrapper">
      <nav>
        <ul>
	    <li><a href="../about.html">about</a></li>
	    <li><a href="../research.html">research</li>
	    <li><a href="./blogs.html">blog</a></li>
	    <li>project</li>
	    <li><a href="https://epsilon-lee.github.io">back</a></li>
	</ul>
      </nav>
      <section>
        <h3>Reread Bengio03 - A neural probabilistic language model</h3>
	<img src="../images/the-starry-night-1889.jpg"/>

      <p>Neural Language Models are now attracting great attention within community of NLP and CL. Neural language model is an analogy from traditional n-gram langauge models. They both model a natural language sequence, most of the time, a sentence, with probability distribution that of generative power. Generative story tell us that we could run computer simulation of a learned n-gram model, to sample instances human probably conceive as sentence.</p> 

      <p>Neural probabilistic language models can do the same things with greater generalization power, longer context and word embeddings as byproduct.</p>

      <p>Yoshua Bengio's 03 paper published on JMLR, summarized back to then, the development of the model, with both statistical and linguistic intuition. I greatly appreciate the <b>Future Work</b> part of the paper, which provided us with opportunity towards very practical, powerful and state-of-the-art model used ubiquitously.</p>

      <p>After rereading his paper, I got stucked about understanding <b>section 5.1 An Energy Minimization Network</b>. I would like to first explain it and then review the intuition viewed from statistics and linguistics.</p>

      

      </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
    
  </body>
</html>
