<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Nov. 28 - Dec.5.2016 - Trivial Matters</title>
<link rel="stylesheet" href="https://stackedit.io/res-min/themes/base.css" />
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body><div class="container"><h1 id="nov28-dec52016-trivial-matters">Nov.28 - Dec.5.2016 - Trivial Matters</h1>

<p><div class="toc">
<ul>
<li><a href="#nov28-dec52016-trivial-matters">Nov.28 - Dec.5.2016 - Trivial Matters</a><ul>
<li><a href="#1-web-crawler">1. Web Crawler</a></li>
<li><a href="#2-generalized-additive-models">2. Generalized Additive Models</a></li>
<li><a href="#3-the-concept-of-smoothing">3. The Concept of Smoothing</a></li>
<li><a href="#4-new-book-drafts-by-dan-jurafsky">4. New Book Drafts by Dan Jurafsky</a></li>
<li><a href="#5-theano-tips">5. Theano Tips</a></li>
<li><a href="#6-dataset">6. Dataset</a></li>
<li><a href="#7-andrew-moore-ml-notes">7. Andrew Moore ML Notes</a></li>
<li><a href="#8-hugos-paper-comments-archive">8. Hugo’s Paper Comments Archive</a></li>
<li><a href="#talks-on-conferences-and-other-scenarios">Talks on Conferences and other scenarios</a></li>
<li><a href="#theano-code-resource">Theano Code Resource</a></li>
<li><a href="#arxiv-weekly">arXiv Weekly</a></li>
<li><a href="#people">People</a></li>
<li><a href="#printed-paper">Printed Paper</a></li>
<li><a href="#programming-tasks">Programming Tasks</a></li>
</ul>
</li>
</ul>
</div>
</p>



<h2 id="1-web-crawler">1. Web Crawler</h2>

<p>A Zhihu post about this topic, <a href="https://www.zhihu.com/question/35461941">https://www.zhihu.com/question/35461941</a>.</p>



<h2 id="2-generalized-additive-models">2. Generalized Additive Models</h2>

<p>Originated from Trevor Hastie, Robert Tibshirani’s co-work.</p>



<h2 id="3-the-concept-of-smoothing">3. The Concept of Smoothing</h2>

<p>Smoothing is an old and classic concept in statistics, see this course note to get a gist of this great idea.</p>

<ul>
<li>5601 Notes: Smoothing: <a href="http://www.stat.umn.edu/geyer/5601/notes/smoo.pdf">http://www.stat.umn.edu/geyer/5601/notes/smoo.pdf</a></li>
</ul>



<h2 id="4-new-book-drafts-by-dan-jurafsky">4. New Book Drafts by Dan Jurafsky</h2>

<ul>
<li><a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a> <br>
<ul><li><strong>Chapter 2. Regular Expression</strong> should be read, since its <strong>PRACTICAL</strong> importance.</li></ul></li>
</ul>



<h2 id="5-theano-tips">5. Theano Tips</h2>

<ul>
<li><p><strong>Shared Variables</strong></p></li>
<li><p><strong><em>borrow</em></strong> argument and speedup</p></li>
</ul>



<h2 id="6-dataset">6. Dataset</h2>

<ul>
<li><a href="http://datasets.maluuba.com/NewsQA"><strong>NewsQA</strong></a> <br>
<ul><li>DeepMind New MachineReading Dataset. (Downloadable.)</li></ul></li>
<li><strong>Maluuba Research</strong>[<a href="http://datasets.maluuba.com/]">http://datasets.maluuba.com/]</a> <br>
<ul><li>They are also working on a <strong>dialogue modeling dataset</strong>, which I could be used to research further on my <strong><em>Learning Linguistic Behavior</em></strong> issue.</li></ul></li>
</ul>



<h2 id="7-andrew-moore-ml-notes">7. Andrew Moore ML Notes</h2>

<ul>
<li><a href="http://www.autonlab.org/tutorials">http://www.autonlab.org/tutorials</a></li>
</ul>



<h2 id="8-hugos-paper-comments-archive">8. Hugo’s Paper Comments Archive</h2>

<ul>
<li>At ShortScience.org: <a href="http://www.shortscience.org/user?name=hlarochelle">http://www.shortscience.org/user?name=hlarochelle</a></li>
<li>I can read one paper and see his comments per day.</li>
</ul>



<h2 id="talks-on-conferences-and-other-scenarios">Talks on Conferences and other scenarios</h2>

<ul>
<li><p><strong>Alan Yuille</strong></p>

<ul><li>Learning Compositional Models <br>
[<a href="http://techtalks.tv/talks/learning-compositional-models/58085/]">http://techtalks.tv/talks/learning-compositional-models/58085/]</a> <br>
The input to computational learning </li>
<li><strong>ICLR talks videos</strong> <br>
<ul><li>Ruslan’s opening talk on <strong>RBM</strong> is very motivating to me. <br>
[<a href="https://sites.google.com/site/representationlearning2013/program-details/program]">https://sites.google.com/site/representationlearning2013/program-details/program]</a></li></ul></li></ul></li>
<li><p><strong>David Blei &amp; John Lafferty</strong> ICML 2016 talk, <strong>Dynamic Topic Models</strong> <br>
[<a href="http://techtalks.tv/talks/dynamic-topic-models/62352/]">http://techtalks.tv/talks/dynamic-topic-models/62352/]</a></p></li>
<li><p><strong>吴立德教授复旦最优化课程</strong> <br>
[<a href="http://list.youku.com/albumlist/show?id=28712545&amp;ascending=1&amp;page=1]">http://list.youku.com/albumlist/show?id=28712545&amp;ascending=1&amp;page=1]</a></p></li>
<li><p><strong>Memory Networks for Natural language understanding</strong>, Jason Weston’s talk at ICML 2016. <br>
[<a href="http://techtalks.tv/talks/memory-networks-for-language-understanding/62356/]">http://techtalks.tv/talks/memory-networks-for-language-understanding/62356/]</a></p>

<ul><li>And other tutorials at the conference <br>
[<a href="http://techtalks.tv/icml/2016/tutorials/]">http://techtalks.tv/icml/2016/tutorials/]</a></li></ul></li>
<li><p><strong>Tutorial KDD 16: Leveraging Propagation for Data Mining: Models, Algorithms and Applications</strong> <br>
[<a href="http://people.cs.vt.edu/~badityap/TALKS/16-kdd-tutorial/]">http://people.cs.vt.edu/~badityap/TALKS/16-kdd-tutorial/]</a></p></li>
</ul>



<h2 id="theano-code-resource">Theano Code Resource</h2>

<ul>
<li><p><strong>Fuel and Block Installation</strong> <br>
[<a href="http://adrianogil.github.io/blog/install-block-fuel-theano.html]">http://adrianogil.github.io/blog/install-block-fuel-theano.html]</a></p></li>
<li><p><strong>Variational AutoEncoder (VAE)</strong></p>

<ul><li><a href="https://github.com/zxie/vae">https://github.com/zxie/vae</a></li>
<li><a href="https://github.com/y0ast/Variational-Autoencoder">https://github.com/y0ast/Variational-Autoencoder</a></li>
<li>Using Theano Blocks: <a href="https://github.com/udibr/VAE">https://github.com/udibr/VAE</a></li></ul></li>
<li><p><strong>Convolutional Neural Networks</strong></p>

<ul><li>Yoon Kim 2014 EMNLP paper “CNN for Sentence Classification” <br>
[<a href="https://github.com/yoonkim/CNN_sentence]">https://github.com/yoonkim/CNN_sentence]</a></li>
<li>LeNet5 simplified (deeplearning.net) <br>
[<a href="http://deeplearning.net/tutorial/lenet.html#lenet]">http://deeplearning.net/tutorial/lenet.html#lenet]</a></li>
<li>CNN 代码详解, 配deeplearning tutorial <br>
[<a href="http://blog.csdn.net/u012162613/article/details/43225445]">http://blog.csdn.net/u012162613/article/details/43225445]</a></li>
<li>TheaNet: CNN for Image Classification <br>
[<a href="https://github.com/rakeshvar/theanet]">https://github.com/rakeshvar/theanet]</a></li></ul></li>
</ul>



<h2 id="arxiv-weekly">arXiv Weekly</h2>

<ul>
<li><p><strong>Reinforcement Learning</strong></p>

<ul><li>Variational intrinsic Control <br>
[<a href="https://arxiv.org/pdf/1611.07507.pdf]">https://arxiv.org/pdf/1611.07507.pdf]</a></li>
<li>Connecting Generative Adversarial Networks and Actor-Critic Methods <br>
[<a href="https://arxiv.org/abs/1610.01945]">https://arxiv.org/abs/1610.01945]</a></li></ul></li>
<li><p><strong>Loss function Modification</strong></p>

<ul><li>Tunable Sensitivity to Large Errors in Neural Network Training <br>
[<a href="https://arxiv.org/pdf/1611.07743.pdf]">https://arxiv.org/pdf/1611.07743.pdf]</a> <br>
This paper propose a <strong>generalized gradient</strong> of cross-entropy function, which is further parameterized to have sensitivity to harder training examples.</li>
<li>jasdf</li></ul></li>
<li><p><strong>Tricks and Optimization in Deep Learning</strong></p>

<ul><li>Tricks from Deep Learning <br>
[<a href="https://arxiv.org/pdf/1611.03777.pdf]">https://arxiv.org/pdf/1611.03777.pdf]</a></li>
<li>Improving Stochastic Gradient Descent with Feedback <br>
[<a href="https://arxiv.org/pdf/1611.01505v1.pdf]">https://arxiv.org/pdf/1611.01505v1.pdf]</a></li></ul></li>
<li><p><strong>Neural Network Architecture</strong></p>

<ul><li>Hierarchical Multi-scale Recurrent Neural Networks <br>
[<a href="https://arxiv.org/pdf/1609.01704v4.pdf]">https://arxiv.org/pdf/1609.01704v4.pdf]</a></li>
<li>Domain Separation Networks <br>
[<a href="https://arxiv.org/pdf/1608.06019v1.pdf]">https://arxiv.org/pdf/1608.06019v1.pdf]</a></li>
<li>Neural Architectures for Named Entity Recognition <br>
[<a href="https://arxiv.org/pdf/1603.01360v3.pdf]">https://arxiv.org/pdf/1603.01360v3.pdf]</a></li>
<li>Neural Multigrid <br>
[<a href="https://arxiv.org/pdf/1611.07661.pdf]">https://arxiv.org/pdf/1611.07661.pdf]</a> <br>
A new CNN architecture design with pyramid spatial structure, significant gain in Semantic Segmentation task.</li></ul></li>
<li><p><strong>Dialogue Modeling</strong></p>

<ul><li>A Simple, Fast Diverse Decoding Algorithm for Neural Generation <br>
[<a href="https://arxiv.org/pdf/1611.08562.pdf]">https://arxiv.org/pdf/1611.08562.pdf]</a></li>
<li>GuessWhat?! Visual object discovery through multi-modal dialogue <br>
[<a href="https://arxiv.org/abs/1611.08481]">https://arxiv.org/abs/1611.08481]</a></li></ul></li>
<li><p><strong>Embed Symbolic Reasoning</strong></p>

<ul><li>Emergent Logical Structure in Vector Representations of Neural Readers <br>
[<a href="https://arxiv.org/pdf/1611.07954.pdf]">https://arxiv.org/pdf/1611.07954.pdf]</a></li></ul></li>
<li><p><strong>Image Captioning</strong> or more broadly <strong>Visual Semantics</strong></p>

<ul><li>Adaptive Feature Abstraction for Translating Video to Language <br>
[<a href="https://arxiv.org/pdf/1611.07837.pdf]">https://arxiv.org/pdf/1611.07837.pdf]</a></li>
<li>Semantic Compositional Networks for Visual Captioning <br>
[<a href="https://arxiv.org/pdf/1611.08002.pdf]">https://arxiv.org/pdf/1611.08002.pdf]</a></li>
<li>Training and Evaluating Multi-modal Word Embeddings with Large-scale Web Annotated Images <br>
[<a href="https://arxiv.org/pdf/1611.08321.pdf]">https://arxiv.org/pdf/1611.08321.pdf]</a></li></ul></li>
<li><p><strong>Linguistic Unit Representation Learning</strong></p>

<ul><li>Unsupervised Learning of Sentence Representation using Convolutional Neural Networks <br>
[<a href="https://arxiv.org/abs/1611.07897]">https://arxiv.org/abs/1611.07897]</a></li>
<li>Learning Python Code Suggestion With a Sparse Pointer Network <br>
[<a href="https://arxiv.org/pdf/1611.08307.pdf]">https://arxiv.org/pdf/1611.08307.pdf]</a></li>
<li>Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling <br>
[<a href="https://arxiv.org/pdf/1611.08034.pdf]">https://arxiv.org/pdf/1611.08034.pdf]</a></li>
<li>Neural Machine Translation with Latent Semantics of Image and Text <br>
[<a href="https://arxiv.org/pdf/1611.08459.pdf]">https://arxiv.org/pdf/1611.08459.pdf]</a></li>
<li>Bidirectional Tree-Structured LSTM with Head Lexicalizationi <br>
[<a href="https://arxiv.org/pdf/1611.06788.pdf]">https://arxiv.org/pdf/1611.06788.pdf]</a></li></ul></li>
<li><p><strong>Deep Generative Models</strong></p>

<ul><li>Deep Restricted Boltzmann Networks <br>
[<a href="https://arxiv.org/pdf/1611.07917.pdf]">https://arxiv.org/pdf/1611.07917.pdf]</a></li>
<li>Conditional Image Synthesis With Auxiliary Classifier GANs <br>
[<a href="https://arxiv.org/abs/1610.09585]">https://arxiv.org/abs/1610.09585]</a></li>
<li>Connecting Generative Adversarial Networks and Actor-Critic Methods <br>
[<a href="https://arxiv.org/abs/1610.01945]">https://arxiv.org/abs/1610.01945]</a></li></ul></li>
<li><p><strong>Machine Reading</strong></p>

<ul><li>Long Short-Term Memory-Networks for Machine Reading <br>
[<a href="https://arxiv.org/abs/1601.06733]">https://arxiv.org/abs/1601.06733]</a></li></ul></li>
<li><p><strong>Others</strong></p>

<ul><li>Unsupervised Learning for Lexicon-Based Classification <br>
[<a href="https://arxiv.org/pdf/1611.06933.pdf]">https://arxiv.org/pdf/1611.06933.pdf]</a></li></ul></li>
</ul>



<h2 id="people">People</h2>

<ul>
<li><p><a href="http://ttic.uchicago.edu/~haiwang/">Hai Wang</a></p>

<ul><li>PhD student at TTI-Chicago. He has a lot interests in Natural Language Processing especially machine reading. Read the following paper by him. <br>
<ul><li><a href="http://openreview.net/pdf?id=ryWKREqxx">Emergent logical structure in vector representations of neural reader</a>, ICLR 2017 under review.</li>
<li><a href="http://arxiv.org/abs/1608.05457">Who did what: a large-scale persoin-centered cloze dataset</a>, EMNLP 2016.</li>
<li><a href="http://ttic.uchicago.edu/~haiwang/publication/GLM_aistats2015.pdf">Inferring Block Structure of Graphical Models in Exponential Families</a>, AISTATS 2015. </li></ul></li></ul></li>
<li><p><a href="http://ttic.uchicago.edu/~hmei/">Hongyuan Mei</a></p>

<ul><li>The <strong>same age as myself</strong>, after a year visit at TTI-Chicago, he now joined JHU as a first year PhD student.</li>
<li>He has worked in Natural Language Generation and NL to Action Modeling in robotic navigation: <br>
<ul><li><a href="http://arxiv.org/abs/1509.00838">What to talk about and how? Selective generation using LSTMs with coarse-to-fine alignment</a>, NAACL 2016.</li>
<li><a href="http://arxiv.org/abs/1506.04089">Listen, attend, and walk: neural mapping of navigational instructions to action sequences</a>, AAAI 2016.</li></ul></li></ul></li>
<li><p><a href="http://www.foldl.me/">Jon Gauthier</a></p>

<ul><li>PhD candidate at Stanford, supervised by Chris Manning. He now joined OpenAI as an Intern. He claims in his homepage that he is willing to <strong>help ambitious undergraduate students succeed</strong>, so if it is possible I’d like to get in touch with him with <strong>jon@gauthiers.net</strong>.</li>
<li>His new paper with Sam Bowman at ACL 2016 is a must read: <br>
<ul><li><a href="http://www.foldl.me/uploads/papers/nips-main2016.pdf">A fast unified model for parsing and sentence understanding</a>.</li>
<li>Another notice is that he has a the same idea as I do, the embodied cognition view of machine learning. He will give a talk at NIPS’ workshop, with the title <a href="http://www.foldl.me/uploads/papers/nips-main2016.pdf">A paradigm for situated and goal-driven language learning</a>.</li></ul></li></ul></li>
<li><p><a href="https://sites.google.com/site/larssonstaffan/">Staffan Larsson</a></p>

<ul><li>Professor of computational linguistics at the Department of Philosophy, Linguistics, and Theory of Science at the University of Gothenburg. His representative work is <strong>Dialogue Management</strong>, and his PhD thesis “Issue-based Dialogue Management” is really a worth reading. He is also a member of the editorial board of the Journal <a href="http://www.dialogue-and-discourse.org/">Dialogue and Discourse</a>.</li>
<li>He is part of a Dialogue System Project named <a href="http://www.ling.gu.se/~cooper/records/"><strong>“Records and Dialogue”</strong></a>, this project is aimed at developing a <strong>formal compositional semantics of dialogue dynamics</strong>. </li></ul></li>
<li><p><a href="https://sites.google.com/site/olemon/">Oliver Lemon</a></p>

<ul><li>Director of the Interaction Lab in the Department of Computer Science at Heriot-Watt University, Edinburgh, working in AI and NLP. He previously worked at Edinburgh and Stanford Universities. </li>
<li>“I work on machine learning methods for intelligent and adaptive multimodal interfaces, including Speech Recognition, Spoken Language Understanding, Natural Language Generation, and Human-Robot Interaction. I have also applied this work in Technology Enhanced Learning using Virtual Character.”</li>
<li>He has pioneered work on Dialogue System with Reinforcement Learning.</li></ul></li>
<li><p><a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></p>

<ul><li>She is interested in building machines that understand the stories that videos portray, and, inversely, on using videos to teach machines about the world. She is now the Assistant Professor at CMU, Machine Learning Department.</li>
<li>Her new course <strong><em>“Deep Reinforcement Learning and Control”</em></strong> would be deadly expected by myself.</li>
<li>One paper is of my interest: <br>
<ul><li><a href="https://www.cs.cmu.edu/~katef/papers/ICCV2015_humandynamics.pdf">Recurrent Network Models for Human Dynamics</a>, ICCV 2015.</li></ul></li></ul></li>
<li><p><a href="http://shangtongzhang.github.io/">Shangtong Zhang</a></p>

<ul><li>Born in 1993.Nov. Bachelor Student at Fudan Univ. (2012-2016)</li>
<li>Very keen on C++ programming, and has lot of coding experience!</li>
<li>Now MSE project at Alberta University doing <strong>Reinforcement Learning</strong>.</li></ul></li>
<li><p><a href="http://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a></p>

<ul><li>Assistant Professor at JHU. Specialty in Knowledge Acquisition.</li>
<li>Some papers of my interests: <br>
<ul><li><a href="http://www.cs.jhu.edu/~vandurme/papers/Universal-Decompositional-Semantics-on-Universal-Dependencies--EMNLP-2016.pdf">Universal Decompositional Semantics on Universal Dependencies</a>, EMNLP 2016.</li>
<li><a href="https://arxiv.org/pdf/1610.01901.pdf">Discriminative Information Retrieval for Knowledge Discovery</a>, 2016.</li></ul></li>
<li>There are some datasets associated with his research, such as FrameNet+, PPDB for Paraphrase etc.</li></ul></li>
<li><p><a href="http://iiis.tsinghua.edu.cn/~jianli/index.htm">Jian Li</a></p>

<ul><li>Theoretical computer science (background in Database Theory and System), now work with Yao at Tsinghua Univ. He has many theoretical interests in CS, including algorithms, online learning, algorithmic game theory etc.</li>
<li>He has a course <a href="http://iiis.tsinghua.edu.cn/~jianli/courses/ML2016/deeplearning.htm"><strong>Deep Learning</strong></a> is worth reading its course materials.</li></ul></li>
</ul>



<h2 id="printed-paper">Printed Paper</h2>

<ul>
<li><strong>A Unified Bayesian Model of Scripts, Frames and Language</strong>, AAAI 2016, JHU </li>
<li><strong>Generative Models of Monolingual and Bilingual Gappy Patterns</strong>, 2011.</li>
</ul>



<h2 id="programming-tasks">Programming Tasks</h2>

<ol>
<li>Use python lib matplotlib to draw 12 common distributions.</li>
<li>Python高级编程技巧 (in Chinese): <a href="http://blog.jobbole.com/61171/">http://blog.jobbole.com/61171/</a>, may take a short look.</li>
</ol></div></body>
</html>